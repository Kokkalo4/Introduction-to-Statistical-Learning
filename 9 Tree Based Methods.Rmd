Decision Trees
===============

We will have a look at the `Carseats` data using the `tree` package in R, as in the lab in the book. 
We create a binary response variable `High`(for high sales) and we include it in the same dataframe.

```{r}
require(ISLR)
require(tree)
attach(Carseats)
hist(Sales)
High <- ifelse(Sales <= 8, "No","Yes")
Carseats <- data.frame(Carseats, High)
```

Now we fit a tree to these data, and summarize and plot it. Notice that we have _exclude_ `Sales` from the right hand side of the formula, because the response is derived from it.

```{r}
tree.carseats <- tree(High ~ . -Sales , data = Carseats) #Binary tree that will have a yes or no output at the terminal nodes because its a binary tree(High has 2 values, yes and no)
summary(tree.carseats)
plot(tree.carseats) 
text(tree.carseats, pretty = 0) #after plotting the tree we put text on it. Of course in this case it's really bad(big letters and shit). Output on the terminal nodes is yes or no as it is a binary tree, and above the nodes is the value where the split occured.
```

For a detailed summary of the tree, print it

```{r}
tree.carseats #Detailed version of the tree, It gives us the details of every single terminal node. The root tells you how many observations were at the root, what's the mean deviance of the root, and then for every single splitting variable(a proportion of yeses and nos of the root) a single node is numbered and gives you details about the particular split. 
```

Lets create a training and test set(250, 150) split of the 400 observations, grow the tree on the training set and evaluate its performance on the test set.

```{r}
set.seed(1011) #set seed to make results reproducible.
train <- sample(1:nrow(Carseats), 250) #we sample to make a train set. 
tree.carseats <- tree(High ~ . -Sales, Carseats, subset = train) #we refit our model using the same formula as above, except we tell the tree to use a subset that equals train.
plot(tree.carseats); text(tree.carseats, pretty = 0) #We plot the tree, and input the text.
tree.pred <- predict(tree.carseats, Carseats[-train,], type = "class") #We take the tree created above and we use it to predict on the test set. And we use the predict method for trees. To do so we use predict() function, where we first input the tree created with train data, then we put the test data(the data we want to predict) and we also tell it that `type = "class"` because in this case we want to predict the class labels. 
with(Carseats[-train,], table(tree.pred, High)) #Now we want to evaluate the error, so we use the table(tree.pred, High) where tree.pred is what we just predicted above with the test data , and High exists on the train data. The output of this command is a misclassification table, on the diagonals are the correct classifications and off diagonal are the incorrect classifications. 
(72 + 33)/150 #So here we get the number of correct(diagonals) and we sum them up and divide them with the total number of observations and we get an error rate of 0.7 with this tree.
```

The tree was grown to full depth, and might be too variable. We now use CV to prune it.

```{r}
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass) #So now we are going to use cross-validation to prune the tree optimally, and so we do that by using the cv.tree. And we tell it that we want to use the misclassification error as the basis for doing the pruning. So this will do ten fold cross-validation for us.
cv.carseats #Here we print out the results. Printing the results tells you some details of the path of the cross validation. First it tells you the size of the trees, it tells you the deviance as the pruning proceeded, and it tells you what the cost complexity parameter was in the process.
plot(cv.carseats) #Next we plot it.
prune.carseats <- prune.misclass(tree.carseats , best = 13) #From the plot above we choose the value , where the tree will be pruned. We choose a value near the minimum(here we choose 13), and we prune our tree to a size of 13 to identify that tree. NOTE: this is a tree fit on the full training data.
plot(prune.carseats); text(prune.carseats, pretty = 0) #We plot the pruned tree and put some text on it. As we can see its a little bit shallower than the trees created previously, and that's the result of cross-validation.
```

Now let's evaluate the pruned tree on the test data.

```{r}
tree.pred <- predict(prune.carseats, Carseats[-train,], type = "class")
with(Carseats[-train,], table(tree.pred, High)) #Results seem to be just like above(before we prune the tree), so in this case we didn't get much from pruning except we got a shallower tree, which is easier to interpret.
(72 + 32)/150 
```
It has done about the same as our original tree. So pruning did not hurt us wrt misclassification errors and gave us a simpler tree.




Random Forests and Boosting
===========================

These methods use trees as building blocks to build more complex models. Here we will use the Boston housing data to explore random forests and boosting. These data are in the `MASS` package. 
It gives housing values and other statistics in each of 506 suburbs of Boston based on a 1970 census.

Random Forests
==============

Random forests build lots of bushy trees, and then average them to reduce the variance.

```{r}
require(randomForest)
require(MASS)
set.seed(101) #set a random seed
dim(Boston) #see the dimensions of Boston data 
train <- sample(1:nrow(Boston),300) #create a train data set(300 values in it)
?Boston #get info on the Boston data.
```

Lets fit a random forest and see how well it performs. We will use the response `medv`, the median housing value( in \$1K dollars)

```{r}
rf.boston <- randomForest(medv ~ . , data = Boston , subset = train) #Here we create the random forest on the training data specified above as a function of medv against all the other variables. 
rf.boston #we print out the random forests which gives us a summary, which tells us the number of trees(500 trees were grown in this case), it also gives you a mean of squared residuals(these are out of bag mean squared residuals) and the percentage variance explained. So each observation was predicted using the average of trees that didnt include it. So these are sort of de-biased estimates of prediction error.
```

The MSR and the % variance explained are based on OOB or _out-of-bag_ estimates, a very clever device in random forests to get honest error estimates.The model estimates that `mtry=4`, which is the number of variables randomly chosen at each split. Since $p = 13$ here, we could try all of the possible values of `mtry`. We will do so, record the results and make a plot.

```{r}
oob.err <- double(13)
test.err <- double(13)
for (mtry in 1:13) {
  fit <- randomForest(medv ~ . , data = Boston, subset = train , mtry = mtry , ntree = 400)
  oob.err[mtry] = fit$mse[400]
  pred <- predict(fit, Boston[-train,]) #predict on test data.
  test.err[mtry] = with(Boston[-train,], mean((medv-pred)^2)) #Compute the test error.
  cat(mtry," ") 
}
matplot(1:mtry, cbind(test.err,oob.err), pch = 19, col = c("red","blue"), type = "b", ylab = "Mean Squared Error")
legend("topright", legend = c("OOB","Test"), pch = 19 , col = c("red","blue"))
```

Not too difficult. Although the test error curve drops below the OOB curve, these are estimates based on data, and so have their own standard errors(which are typically quite large). Notice that the points at the end with `mtry=13` correspond to bagging.


Boosting
========

Boosting builds a lot of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble. 

```{r}
require(gbm)
boost.boston <- gbm(medv ~ . , data = Boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01 , interaction.depth = 4) #We ask for 10000 trees with maximum depth of 4(we want 4 splits on each of the trees as we set interaction.depth = 4).
summary(boost.boston) #summary gives a variable importance plot. 
plot(boost.boston, i = "lstat")
plot(boost.boston, i = "rm")
```
Lets make a prediction on the test set. With boosting, the number of trees is a tuning parameter, and if we have too many we can overfit. So we should use cross validation to select the number of trees. We will leave this as an exercise. Instead we will compute the test error as a function of the number of trees, and make a plot. 

```{r}
n.trees <- seq(from = 100 ,to = 100000 ,by 100)
predmat <- predict(boost.boston, newdata = Boston[-train,] , n.trees = n.trees)
dim(predmat)
berr <- with(Boston[-train,],apply((predmat - medv)^2,2,mean))
plot(n.trees, berr, pch = 19, ylab = "Mean Squared Error", xlab = "# trees", main = "Boosting Test Error")
abline(h = min(test.err),col = "red")